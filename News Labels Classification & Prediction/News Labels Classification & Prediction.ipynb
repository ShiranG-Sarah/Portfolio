{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import nltk\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.politics.guns','rec.sport.baseball'] # We focus on 2 news categories\n",
    "def get_data():\n",
    "    data = fetch_20newsgroups(subset='all',\n",
    "                              shuffle=True,\n",
    "                              categories=categories,\n",
    "                              remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text data and their labels\n",
    "dataset = get_data()\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target\n",
    "\n",
    "# split training dataset and testing dataset\n",
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(corpus,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow features\n",
    "from sklearn.feature_extraction.text import CountVectorizer #tokenizes and counts words\n",
    "\n",
    "# build bag of words features' vectorizer and get features\n",
    "bow_vectorizer=CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #alternatively, use TfidfTransformer()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize documents for word2vec\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                   for text in test_corpus]  \n",
    "\n",
    "# build word2vec model                   \n",
    "wv_model = gensim.models.Word2Vec(tokenized_train,\n",
    "                               vector_size=200, #set the size or dimension for the word vectors. Used to be size = 200\n",
    "                               window=60,       #specify the length of the window of words taken as context\n",
    "                               min_count=10)    #ignores all words with total frequency lower than \n",
    "\n",
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    \n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word]) # Used to be model[word]\n",
    "    \n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "        \n",
    "    return feature_vector \n",
    "   \n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)\n",
    "\n",
    "# averaged word vector features from word2vec\n",
    "avg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\n",
    "                                                 model=wv_model,\n",
    "                                                 num_features=200)                   \n",
    "avg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\n",
    "                                                model=wv_model,\n",
    "                                                num_features=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# define a function to evaluate our classification models based on four metrics\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print ('Accuracy:', np.round(                                                    \n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "    print ('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that trains the model, performs predictions and evaluates the predictions\n",
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)\n",
    "    return predictions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69\n",
      "Precision: 0.72\n",
      "Recall: 0.62\n",
      "F1 Score: 0.66\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate {knn} with {bow} feature\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_bow_predictions = train_predict_evaluate_model(classifier=knn,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  226   68\n",
       "1  107  171"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for knn bow model\n",
    "cm = metrics.confusion_matrix(test_labels, knn_bow_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.53\n",
      "Precision: 0.53\n",
      "Recall: 0.32\n",
      "F1 Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate {knn} with tfidf feature\n",
    "knn_tfidf_predictions = train_predict_evaluate_model(classifier=knn,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  213  81\n",
       "1  188  90"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for knn tfidf model\n",
    "cm = metrics.confusion_matrix(test_labels, knn_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.82\n",
      "Recall: 0.82\n",
      "F1 Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate {knn} with word2vec feature\n",
    "knn_word2vec_predictions = train_predict_evaluate_model(classifier=knn,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  243   51\n",
       "1   50  228"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for knn word2vec model\n",
    "cm = metrics.confusion_matrix(test_labels, knn_word2vec_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball -> talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "# Observe false positive output\n",
    "class_names = dataset.target_names\n",
    "print (class_names[0], '->', class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "   WHO THINKS THE ASTROS ARE GOING PLACES??? THEY'RE CURRENTLY FIRST PLACE. THEY'RE 5-4, 5-1 ON THE ROAD!  \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Has David Wells landed with a team yet?  I'd think the Tigers with their  anemic pitching would grab this guy pronto!\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "     Ted, you're missing a vital point.  As Roger Lustig pointed out in a previous response, the reason why Schott was banned from baseball was because she had been known to call and think in a racially biased manner on a constant basis.  Such thoughts affected her hiring practices.  Bonilla, on the other hand, was found to have mentioned this one word a single time.  If he had been known to go around, criticizing homosexuals, it would be a different story.  Furthermore, he is merely an athlete.  He doesn't have to hire anyone as Schott had to do.  Dave Pallone, the former NL umpire who is an admitted homosexual, has decided to assist in a protest before a Mets game at Shea.  He, like you, thinks that Bonilla should be suspended from baseball.  Pallone is hoping for a year's suspension.  In my opinion, that's downright ludicrous.  As Howie Rose on WFAN said, if you start suspending athletes who have mentioned a derogatory word even a single time under whatever conditions, then you'd probably have enough people remaining to play a three-on-three game.  Now, honestly, if you truly analyze the differences between the two cases that you bring up in your article, I would think that you'd reconsider your thoughts.                                       -Sean  \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "Philadelphia at Chicago:  Teams tied for 1st after Sunday      Dick Redding battled Chet Brewer in the first game of a dramatic four game series.  One Friday, one Saturday, and a good-old Sunday doubleheader. \"What could be better,\" declared Ernie Banks.  Perhaps the fact that the Cubs are challenging?      \"It's pitching, it's always been pitching that we've lacked,\" announced Ryne Sandberg.  \"If we can get by Brewer, then beat Carlton, Alexander, or Bunning - preferrably 2 of the last three - we'll know we might be able to win.      \"Lord, I hope we pull it off.\"      The Phils scored once in the top of the first; Richie Ashburn singled, Pete Rose followed with a hit, sending Ashburn around second.  Kiki Cuyler cut the ball off in left center, and threw a bullet in to Ernie Banks, who threw to Ron Santo to get Ashburn at third.  Rose went to second on the play.      Christobel Torrienti lifted a long fly to center, moving Pete Rose to third.  Schmidt was walked - the Cubs were absolutely refusing to let him beat them.  Both Torrienti and Schmidt will likely draw 130-150 walks this year.  Chuck Klein is starting to hit very well, and he lashed a double into a gap in right-center.  \"Cool Papa\" Bell's speed allowed him to cut the ball off and prevent Schmidt from scoring.  Nellie Fox was walked, and Bob Boone grounded out to second, ending the threat.        \"Teams are starting to realize that you don't have to pitch to Schmidt or Torrienti, and that is lowering their run total.  It puts a lot of pressure on Klein and Dick Allen (who platoons with Chuck Klein and occasionally spells Rose at first), and it's a credit to the Phillies that they've been able to sustain their pace.  The picthers have slumped at times.\"  So came the analysis from Frank Chance.      The Cubs got that run back when Bell bunted for a hit, Thomas' grounder moved him to second, and - after Sandberg made out - Billy Williams singled home a run.  In the sixth, Ron Santo launched a two-run homer to make it 3-1.  Dick Redding got in trouble in the eighth, as Schmidt singled and Klein singled him to third.  Ed Reulbach entered to face Fox, but Dick Allen popped out of the dugout to hit.  Allen doubled to right, but luckily for the Cubs, Williams had moved to left and Andre Dawson had been inserted for defense. He fired a bullet to home plate to keep Klein at third.  Lance Parrish, hitting for Boone, was walked, and Bruce Sutter entered.  Larry Bowa grounded into a 1-2-3 double play, but Ed Delahanty walked as a pinch-hitter.  Desiring a strikeout, since Ashburn was likely to attempt a bunt hit with the quick Trillo pinch-running at third, Chance brought in Lee Smith, who induced a pop-up to the catcher from the speedy centerfielder, ending the inning.  The Cubs took the win, 3-2, moving a game behind the Phillies.      Steve Carlton was called upon to battle 3-Finger Brown Saturday.  To get another righthander in the lineup, Ron Santo moved to first and Bill Madlock played third.  Unfortunately, Brown allowed six doubles, and the Cub bullpen was worn down even more, as the Cubs tried to maintain a lead against Lefty.  Madlock, batting sixth, had knocked two doubles of his own, driving home four runs.  Gabby Hartnett hit two home runs, and Cuyler added another, and the score was 8-6, Cubs after six innings.  The Phillie bullpen had more troubles in the bottom of the eighth, as the Cubs grabbed 3 more runs to ice an 11-7 triumph.      Sunday's twin bill saw Cool Papa Bell gather seven straight hits at one point, including a rare outside-the-park home run in the second game, off Robin Roberts.  Grover Alexander of the Phils took the first contest, 4-2, but the Cubs captured the second one 5-4, with Waddell gaining the win.  Bruce Sutter tossed two innings for the save, though he allowed one run in the eighth.  The Cardinals stood half a game behind these co-leaders, and would conclude their series with the Expos on Monday. -------------------------------------------------------------------      Montreal at St. Louis(August 3-6): 3-way tie for 1st      Dennis Martinez is on a roll, and he continued it versus John Tudor Friday. The Expos have a wide variety of hitters, and - while they aren't among the all-time greats, they are getting the job done.  After winning their first first two games, they suddenly found themselves only 2 1/2 games out of first in this wacky season.      Martinez triumphed 5-3 on Friday, and WIlliams outdueled Dizzy Dean 3-2 Saturday.  However, the Cardinals refused to give up, winning 6-2 on Sunday. The Cards captured Monday's game, too, as Steve Carlton outdueled Steve Rogers 3-2.  \"We're really good against ground ball pitchers because of our team speed,\" remarked Lou Brock.  \"I don't see why we can't win this division.\"      The Phillies and Cubs may have some reasons for them.  Two-thirds of the way through the season, there is a 3-way tie for first. -------------------------------------------------------------------      New York at Pittsburgh(August 3-6): 3 straight 3-2 wins for Bucs, now 2 back - but in 4th!     \"When your team is in a slump like we are, the worst thing is to play in a pitcher's park like this,\" spoke Gil Hodges before the series.  Keith Hernandez added that \"their defense takes away quite a few runs per year, and it must be giving them an extra 6-7 wins.\"  The Pirates have made only 26 errors all season, 6 ahead of the second place Dodgers.  Error totals tend to be around 50 for the best defensive All-Time teams.      Rube Foster defeated Sid Fernandez 5-2 Friday, and Candelaria outshone Seaver 3-2 Saturday, in a game featuring some outstanding defense.  When Nolan Ryan and two relivers 6-hit the Mets in another 3-2 win Sunday, the Pirates could once again look forward to a victory getting them back to the .500 mark.  They had been unable to several times in the past month.  Bert Blyleven met Dwight Gooden in the afternoon game.  Both pitchers possessed fantastic stuff, and the only runs scored through eight innings were on home runs - a solo shot by Rusty Staub of the Mets and a two-run blast by Ralph Kiner for the Pirates.  The Mets' Darryl Strawberry singled home a run in the top of the ninth off Jesse Orosco, working his second inning, after Mookie Wilson pinch-ran for Gary Carter at second.  With one out and a runner on first, Lee Mazilli was sent in to pinch-hit.  The Pirates countered with Kent Tekulve, placing him in the fifth spot in the order and putting Barry Bonds in left field as the ninth place hitter.  Tekulve induced a groundout forcing Strawberry at second.  He slid hard into Honus Wagner, preventing the Pirates from turning their fifth double play of the afternoon.  Tekulve allowed a hit, but Clemente threw Mazilli out at third from near the right field line, ending the inning.  Tug McGraw relieved Randy Myers, who entered to pitch the eighth, and got one out before Bonds launched a rocket to deep center, running through the stop sign at third to score an inside-the-park homer to win.  The Pirates had scored an improbable 3 straight 3-2 wins, and had moved to within 2 games of first place, with seven weeks to go. ------------------------------------------------------------------      San Francisco at Boswaukta(August 3-5):      Another Sunday doubleheader appeared on the schedule, as the Giants managed to close the gap on the other teams thanks to some starting pitching that just wouldn't tire.  In fact, reported manager John McGraw, \"once this rough part of the schedule is over, maybe as early as this coming week, we may shift to a 4-man rotation again for a little while.\"      Juan Marichal continued his hot pitching Friday, beating Lew Burdette and the Braves 4-1.  Willie Mays had all four r.b.i.s on 3 hits.  Rick Reuschel faced Joe Niekro Saturday in a slugfest.  The Braves' park had been a homer haven, but this took the cake, as the Giants won a seesaw affair 16-13. Willie Mays had three homers, Willie McCovey, Eddie Matthews, and Don Baylor had two, and Hank Aaron, Ernie Lombardi, Biz Mackey, and Mel Ott had one each.  The Braves had collected 149 home runs going into Sunday's doubleheader, putting them on a pace for 223, which would be 4 short of the National League record.  They were still a tad behind the '61 Yankees' pace. They had allowed over 120, though.  Vida Blue actually got the win after retiring 2 batters in the fifth.  He allowed only a run in the sixth, but faltered in the seventh.  Joe McGinnity earned the save.      In the doubleheader, the Braves' Hoyt Wilhelm failed to hold a lead in the first game, but Hank Aaron homered off Bill Foster in the eighth as the Braves won, 4-3.  The Giants took the second game, however, by a 6-2 score. The homer by Aaron was a magical #150 by the Braves; however, they fell to three game below .500, making a comeback extremely unlikely. -------------------------------------------------------------------      San Diego at Cincinnati(Aug. 3-5):  Randy Jones faced Ewell Blackwell in the first of this 3-game series, and the Padres felt rather good.  With Don Mattingly straining his back in the last Cleveland game, the trade looked even better.  McGriff's batting average was even rising.  Of course, the bench was very poor, and Joe Gordon was only adequate in the outfield, but these were minor problems, since the pitching was holding up.      Jones pitched a good game Friday, and won 6-3.  McGriff launched two home runs.  Mel Harder earned a win with the help of Mark Davis and Ray Narleski Saturday; 5-4 was the final score.  Tom Candiotti battled Satchel Paige to a 3-3 tie through eight innings before departing.  The game was scoreless for 4 more innings until the thirteenth.  Paige had departed after 10, and John Franco hurled a scoreless inning.  Tom Browning was working his second scoreless inning, when Dave Winfield doubled with one out and Joe Gordon was pitched around.  Thurm Munson doubled both runners home, and the Padres gamed a 5-3 win.  The three-game sweep had pulled the Giants into a tie with the Reds.  Though the Reds denied it, the highly emotional series with the Dodgers may have taken too much out of them. -------------------------------------------------------------------      Brookangeles at Houston(August 3-5):      Another series capped off by a weekend doubleheader took place in the wide open plains of the Astrodome.  The Astros sent Joe Niekro to the hill in the first game, opposite Don Drysdale.  \"Normally,\" Drysdale remarked, \"I would be challenging hitter by being ready to throw at them.  I can't afford to with this team, though; we have to get our own runners going; we can't Davis will get decked once.\"      The fact that Glenn Davis leads the team in homers with six (!) is primarily why he would be decked, but it should be understood that his current pace would give him nine for the season.  The hitting on this team is a little better, but the power is all doubles and triples.  Still, Carl Furillo is the main reason no Astro home runs were hit over the weekend, as he threw two runners out trying for inside-the-park homers.  For those unaware of the nastness of the Astros' park, they have a 23-foot high gray wall all around the outfield; balls must be hit into the seats to be home runs.  The foul poles are 355 feet from home plate, but the alleys are 400 feet away, with center field at 420 feet.  \"It's as if some three-year-old threw a tantrum and told his playmates: 'If I can't hit home runs, nobody will hit home runs',\" remarked Roy Campanella.      The Dodgers stole five bases Friday, but the Astros decided to revitalize the Baltimore chopping that had failed 6 weeks before; for tonight, anyway, Davis, Jim Wynn, and Jose Cruz did not have to mess with their swings, according to the manager.  After Poles and Willie Wells reached base via the Baltimore chop, Drysdale decked Jose Cruz with a pitch.  He responded with a two-run double, but Wynn - playing first for Davis - popped up, and the Astros didn't score any more in that inning.  They did score 3 in the fourth to erase a 3-2 deficit, and the Astros wound up winning 6-4.  They threatened to do even better the next game, as Tommy John would be their opponent. Walt Alston met privately with the starters at 6 A.M. before the game.      \"I think I know how we can beat the Baltimore Chop,\" he explained.      \"How can we do that,\" Pee Wee Reese wondered.      \"They're going to be beating the ball down, so we've got to be ready to throw on the run.  Steve will start at first to dig balls out of the dirt, but I want all of you to practice your barehanded picks and throws.  We'll go with a shallow infield almost the whole time.\"      The plan almost worked.  Mike Scott allowed only two runs through eight innings, but the Astros got three; two of them scored when Bill Doran pushed a bunt into the outfield in the fourth with runners on second and third.  3-2 Astros was the final, with Dave Smith earning another save.  The Dodgers scored a victory in the first game of the twin bill Sunday, as Nolan Ryan walked five, three of whom scored in a 4-1 Dodger win.  Fernando Valenzuela lost the second game 4-2 to Don Sutton, however, as the Dodgers' thirteen stolen bases in the series proved to not be enough.      \"We're mostly a power team,\" remarked Ron Cey.  \"Jackie and, when he plays, Maury Wills are our only real speed demons, though a couple other plays can do it now and then.  We're sunk in a place like the Astrodome.  I guess that's why they're so successful there.\"  Indeed, it seems that basestealing teams give them the most trouble in the dome.  The 'Stros swiped 12 bases in 16 attempts, giving them 230 on the season.      Standings after these weekend series: A.L.East Team                     W           L           GB New York                68        42          -- Cleveland               65        46          3.5 Detroit                 64        46           4 Boston                  64        47          4.5 Baltimore               59        52         9.5 Toronto                 43         69        25.5 Washington              39         73          28  A.L.West Oaksaselphia            63         48          -- Minnesota               61         48            1 Chicago                 59         53          4.5 Kansas City             57         54          6.5 California              57         56            8 Milwaukee               45         66           17 Seattle                 32         78         31.5  N.L.East Chicago          57         53                -- Philadelphia     58         54                -- St. Louis        58         54                -- Pittsburgh       56         56                2 Montreal         53         56               3.5 New York         48         64               10  N.L.West Brookangeles     66          46               -- Cincinnati       66           47              0.5 San Francisco    65          46               0.5 Boswaukta        54           56               11 Houston          50          61               17.5 San Diego        36          75                29.5\n"
     ]
    }
   ],
   "source": [
    "# Look at some misclassified documents in detail\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, knn_bow_predictions):\n",
    "    if label == 0 and predicted_label == 1:\n",
    "        print ('Actual Label:', class_names[label])\n",
    "        print ('Predicted Label:', class_names[predicted_label])\n",
    "        print ('Document:-')\n",
    "        print (re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'precomputed']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sklearn.neighbors.VALID_METRICS_SPARSE['brute']) # Get the list of avaliable metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Precision: 0.81\n",
      "Recall: 0.97\n",
      "F1 Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the improved knn with bow features\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='cosine') #Changed the metric to cosine\n",
    "knn_tfidf_predictions = train_predict_evaluate_model(classifier=knn,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  232   62\n",
       "1    8  270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build confusion matrix for knn bow model\n",
    "cm = metrics.confusion_matrix(test_labels, knn_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_predictions\n",
      "Accuracy: 0.95\n",
      "Precision: 0.95\n",
      "Recall: 0.95\n",
      "F1 Score: 0.95\n",
      "\n",
      "svm_bow_predictions\n",
      "Accuracy: 0.91\n",
      "Precision: 0.91\n",
      "Recall: 0.9\n",
      "F1 Score: 0.9\n",
      "\n",
      "svm_tfidf_predictions\n",
      "Accuracy: 0.92\n",
      "Precision: 0.95\n",
      "Recall: 0.88\n",
      "F1 Score: 0.92\n",
      "\n",
      "svm_avgwv_predictions\n",
      "Accuracy: 0.81\n",
      "Precision: 0.76\n",
      "Recall: 0.9\n",
      "F1 Score: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Other approaches\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=100)\n",
    "\n",
    "# Multinomial Naive Bayes with bag of words features\n",
    "print('mnb_bow_predictions')\n",
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "#Support Vector Machine with bag of words features'\n",
    "print('\\nsvm_bow_predictions')\n",
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with tfidf features\n",
    "print('\\nsvm_tfidf_predictions')\n",
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)\n",
    "\n",
    "# Support Vector Machine with averaged word vector features\n",
    "print('\\nsvm_avgwv_predictions')\n",
    "svm_avgwv_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=avg_wv_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=avg_wv_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.baseball -> talk.politics.guns\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      " I DID NOT WRITE THAT!  In fact, those statements were a rebuttal to an earlier posting that I made, and this was culled from my *strong* rebuttal to those statements.  PLEASE!  Slander.  Shame.     \"after the game, it's no big deal\" ????   After the employees leave the workplace, it doesn't matter what they say about the boss or the company?  Puhlease.     First, it's Ross Porter.  Second, I am really tired of seeing the kind of response that indicates that all I do is parrot what some media person says or writes.  I have a brain.  If I choose to characterize something in a certain fashion, it's because that is what I believe to be accurate.  It is not just because some unnamed \"mediot\" made the characterization.     I did *not* brand Darryl's response as petulant, because I never heard any response from Darryl.  I did call him a name.  I referred to him  as a primadonna.  Someone else concluded that I did that because I \"hate\" him.  I don't hate him.  I think he's a primadonna.  If you disagree, fine.  But stop putting words in my mouth.    --\tThe Beastmaster  \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "The Royals are darkness.  They are the void of our time. When they play, shame descends upon the land like a cold front from Canada.   They are a humiliation to all who have lived and all who shall ever live.   They are utterly and completely doomed.  Other than that, I guess they're OK.  -- \n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      "  I was in fact going to suggest that Roger take his way of discussion over to r.s.football.pro. There this kind of hormone-only reasoning is the standard. Being he canadian, and hockey what it is, I would have suggested that r.s.h would work too. It is important in a thread that everyone involved use the same body part to produce a post (brain being the organ of choice here).\n",
      "Actual Label: rec.sport.baseball\n",
      "Predicted Label: talk.politics.guns\n",
      "Document:-\n",
      " [Some discussion about whether Elias is money grubbing deleted]      Some thoughts and facts,  1.)  Bill James is a partial owner of STATS, inc.  However he has almost nothing to do with the day-to-day operations of the company, although he does have significant input into the design of the books that bear his name. (The handbook, but not the scoreboard).  To the best of my knowledge, the only things that Bill actually writes for STATS are the predictions section of the handbook, and the Bill James Fantasy Baseball rulebook.  2.) The debate over Elias goes way back.  Bill James' early stuff was hampered by the fact that Elias would not give access to their stats at any price. Project Scoresheet, and later, STATS were founded to fill this void.  You can call STATS, and ask them for a report on just about anything in their database, and they will provide it -- for a price, of course.  Or you could just log into their online system and look at the data yourself.  Having attempted to pry numbers from Elias in the past (football, not baseball), they just don't do that.  In STATS eyes, the high ground comes from making the information available at all.  3.)  That being said, I'm pretty dissapointed by Bill's book this year, too. I am given to understant that it was mostly a response to the publishers  desire to have the book come out sooner than April.  Hope this makes things just a little bit clearer.  (Bias alert.  I am a former part-time employee of STATS.)\n"
     ]
    }
   ],
   "source": [
    "# build confusion matrix for SVM TF-IDF-based model\n",
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "pd.DataFrame(cm, index=range(0,2), columns=range(0,2))  \n",
    "\n",
    "# Observe false positive output\n",
    "class_names = dataset.target_names\n",
    "print (class_names[0], '->', class_names[1])\n",
    "\n",
    "# Look at some misclassified documents in detail\n",
    "import re\n",
    "\n",
    "num = 0\n",
    "for document, label, predicted_label in zip(test_corpus, test_labels, svm_tfidf_predictions):\n",
    "    if label == 0 and predicted_label == 1:\n",
    "        print ('Actual Label:', class_names[label])\n",
    "        print ('Predicted Label:', class_names[predicted_label])\n",
    "        print ('Document:-')\n",
    "        print (re.sub('\\n', ' ', document))\n",
    "        num += 1\n",
    "        if num == 4:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
